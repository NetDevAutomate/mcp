# AWS Labs CI Pipeline - Automated Code Review with Claude Sonnet 4
# GitHub Copilot-style automated code review using litellm proxy

name: Claude Code Review

on:
  pull_request:
    branches: [main, develop]
    types: [opened, synchronize, reopened]
  push:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      target_path:
        description: 'Target project path to review'
        required: false
        default: '.'
      model:
        description: 'AI model to use for review'
        required: false
        default: 'claude-sonnet-4'
        type: choice
        options:
          - claude-sonnet-4
          - claude-opus-4.1
          - nova-premier
          - deepseek-r1

env:
  # LiteLLM proxy configuration
  OPENAI_API_BASE: http://localhost:4040/v1
  OPENAI_API_KEY: sk-litellm-bedrock-proxy-2025
  CLAUDE_MODEL: ${{ inputs.model || 'claude-sonnet-4' }}
  PROJECT_PATH: ${{ vars.PROJECT_PATH || github.workspace }}
  TARGET_PATH: ${{ inputs.target_path || '.' }}

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  validate-proxy:
    name: 🔍 Validate LiteLLM Proxy
    runs-on: ubuntu-latest
    outputs:
      proxy-available: ${{ steps.check-proxy.outputs.available }}
      available-models: ${{ steps.check-proxy.outputs.models }}
    steps:
      - name: Check LiteLLM proxy availability
        id: check-proxy
        run: |
          echo "🔍 Checking LiteLLM proxy at http://localhost:4040"

          # Check if proxy is accessible
          if curl -s --max-time 10 "http://localhost:4040/health" > /dev/null 2>&1; then
            echo "✅ LiteLLM proxy is accessible"
            echo "available=true" >> $GITHUB_OUTPUT

            # Get available models
            MODELS=$(curl -s -H "Authorization: Bearer $OPENAI_API_KEY" \
              "http://localhost:4040/v1/models" | jq -r '.data[].id' | head -10 | tr '\n' ',' || echo "")
            echo "models=$MODELS" >> $GITHUB_OUTPUT

            echo "📋 Available models: $MODELS"
          else
            echo "⚠️ LiteLLM proxy not accessible - skipping AI review"
            echo "available=false" >> $GITHUB_OUTPUT
            echo "models=" >> $GITHUB_OUTPUT
          fi

  detect-changes:
    name: 🔍 Detect Code Changes
    runs-on: ubuntu-latest
    outputs:
      has-changes: ${{ steps.changes.outputs.has-changes }}
      changed-files: ${{ steps.changes.outputs.files }}
      project-type: ${{ steps.analyze.outputs.project-type }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changes
        id: changes
        run: |
          echo "🔍 Detecting code changes..."

          # For push events, compare with previous commit
          if [ "${{ github.event_name }}" = "push" ]; then
            CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD | grep -E '\.(py|yml|yaml|toml|md)$' || echo "")
          # For PR events, compare with base branch
          elif [ "${{ github.event_name }}" = "pull_request" ]; then
            CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | grep -E '\.(py|yml|yaml|toml|md)$' || echo "")
          else
            # Manual trigger - review entire target
            CHANGED_FILES=$(find ${TARGET_PATH} -name "*.py" -o -name "*.yml" -o -name "*.yaml" -o -name "*.toml" -o -name "*.md" | head -20)
          fi

          if [ -n "$CHANGED_FILES" ]; then
            echo "✅ Found changes to review:"
            echo "$CHANGED_FILES" | sed 's/^/  - /'
            echo "has-changes=true" >> $GITHUB_OUTPUT
            echo "files<<EOF" >> $GITHUB_OUTPUT
            echo "$CHANGED_FILES" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No relevant files changed"
            echo "has-changes=false" >> $GITHUB_OUTPUT
            echo "files=" >> $GITHUB_OUTPUT
          fi

      - name: Analyze project type
        id: analyze
        run: |
          echo "🔍 Analyzing project type..."

          PROJECT_TYPE="generic"

          if [ -f "${TARGET_PATH}/pyproject.toml" ]; then
            if grep -q "mcp" "${TARGET_PATH}/pyproject.toml" 2>/dev/null; then
              PROJECT_TYPE="aws-mcp-server"
            elif grep -q "aws" "${TARGET_PATH}/pyproject.toml" 2>/dev/null; then
              PROJECT_TYPE="aws-python"
            else
              PROJECT_TYPE="python-package"
            fi
          elif [ -f "${TARGET_PATH}/package.json" ]; then
            PROJECT_TYPE="nodejs"
          elif [ -f "${TARGET_PATH}/Cargo.toml" ]; then
            PROJECT_TYPE="rust"
          elif [ -f "${TARGET_PATH}/go.mod" ]; then
            PROJECT_TYPE="golang"
          fi

          echo "📋 Detected project type: $PROJECT_TYPE"
          echo "project-type=$PROJECT_TYPE" >> $GITHUB_OUTPUT

  ai-code-review:
    name: 🤖 AI Code Review (${{ matrix.review-type }})
    runs-on: ubuntu-latest
    needs: [validate-proxy, detect-changes]
    if: needs.detect-changes.outputs.has-changes == 'true' && needs.validate-proxy.outputs.proxy-available == 'true'

    strategy:
      fail-fast: false
      matrix:
        review-type: ["security", "quality", "structure", "documentation"]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install httpx rich pydantic

      - name: Run specialized AI review
        id: review
        run: |
          echo "🤖 Running ${{ matrix.review-type }} review with $CLAUDE_MODEL"

          # Create review script
          cat > ai_review_${{ matrix.review-type }}.py << 'EOF'
          import asyncio
          import httpx
          import json
          import sys
          from pathlib import Path

          async def run_review():
              client = httpx.AsyncClient(
                  timeout=300.0,
                  headers={"Authorization": "Bearer ${{ env.OPENAI_API_KEY }}"}
              )

              try:
                  # Specialized prompts based on review type
                  prompts = {
                      "security": """
                      You are a security expert. Review this code for:
                      1. Credential exposure or hardcoded secrets
                      2. SQL injection vulnerabilities
                      3. Command injection risks
                      4. AWS IAM permission issues
                      5. Unsafe deserialization
                      6. Authentication/authorization flaws

                      Focus on AWS CloudWAN/networking security best practices.
                      """,
                      "quality": """
                      You are a code quality expert. Review this code for:
                      1. Code organization and structure
                      2. Error handling patterns
                      3. Performance optimizations
                      4. Type safety and annotations
                      5. Code duplication and refactoring opportunities
                      6. AWS SDK usage best practices
                      """,
                      "structure": """
                      You are a software architect. Review this project for:
                      1. Directory structure and organization
                      2. File naming conventions (flag any with version suffixes)
                      3. Package/module organization
                      4. Separation of concerns
                      5. Dependency management
                      6. Configuration management
                      """,
                      "documentation": """
                      You are a documentation specialist. Review for:
                      1. README completeness and accuracy
                      2. API documentation coverage
                      3. Code comment quality and coverage
                      4. Usage examples and tutorials
                      5. Installation and setup instructions
                      6. Contributing guidelines
                      """
                  }

                  # Read changed files (limited to prevent token overflow)
                  changed_files = """${{ needs.detect-changes.outputs.changed-files }}""".strip().split('\n')
                  code_content = []

                  for file_path in changed_files[:15]:  # Limit files
                      try:
                          if Path(file_path).exists() and Path(file_path).suffix in ['.py', '.md', '.yml', '.yaml', '.toml']:
                              with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                  content = f.read()[:3000]  # Limit content per file
                                  code_content.append(f"## File: {file_path}\n```\n{content}\n```\n")
                      except Exception as e:
                          print(f"Error reading {file_path}: {e}")

                  review_content = '\n'.join(code_content)
                  review_type = "${{ matrix.review-type }}"
                  project_type = "${{ needs.detect-changes.outputs.project-type }}"

                  prompt = f"""
                  {prompts[review_type]}

                  **Project Type**: {project_type}
                  **Review Type**: {review_type}

                  **Code to Review:**
                  {review_content}

                  **Response Format:**
                  Return JSON only:
                  {{
                      "issues": ["List of specific issues found"],
                      "suggestions": ["Actionable improvement suggestions"],
                      "score": 0-100,
                      "summary": "Brief summary of findings"
                  }}
                  """

                  response = await client.post(
                      "${{ env.OPENAI_API_BASE }}/chat/completions",
                      json={
                          "model": "${{ env.CLAUDE_MODEL }}",
                          "messages": [
                              {"role": "system", "content": f"You are an expert {review_type} reviewer for AWS/Python projects. Return only valid JSON."},
                              {"role": "user", "content": prompt}
                          ],
                          "temperature": 0.1,
                          "max_tokens": 2000
                      }
                  )

                  if response.status_code == 200:
                      result = response.json()
                      content = result["choices"][0]["message"]["content"]

                      # Clean up content to extract JSON
                      if "```json" in content:
                          content = content.split("```json")[1].split("```")[0]
                      elif "{" in content:
                          start = content.find("{")
                          end = content.rfind("}") + 1
                          content = content[start:end]

                      try:
                          review_result = json.loads(content)
                          print(f"✅ {review_type.title()} review completed")
                          print(f"📊 Score: {review_result.get('score', 'N/A')}/100")

                          # Save results
                          with open(f"review-{review_type}.json", "w") as f:
                              json.dump(review_result, f, indent=2)

                          return 0

                      except json.JSONDecodeError as e:
                          print(f"❌ JSON parsing failed: {e}")
                          print(f"Raw content: {content[:500]}")
                          return 1
                  else:
                      print(f"❌ API request failed: {response.status_code}")
                      return 1

              except Exception as e:
                  print(f"❌ Review failed: {e}")
                  return 1
              finally:
                  await client.aclose()

          if __name__ == "__main__":
              sys.exit(asyncio.run(run_review()))
          EOF

          # Run the review
          python ai_review_${{ matrix.review-type }}.py

      - name: Upload review results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: review-results-${{ matrix.review-type }}
          path: review-${{ matrix.review-type }}.json
          retention-days: 30

  consolidate-reviews:
    name: 📋 Consolidate Review Results
    runs-on: ubuntu-latest
    needs: [ai-code-review, detect-changes]
    if: always() && needs.detect-changes.outputs.has-changes == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all review results
        uses: actions/download-artifact@v4
        with:
          pattern: review-results-*
          merge-multiple: true

      - name: Consolidate and analyze results
        run: |
          echo "📋 Consolidating review results..."

          # Create consolidated report
          cat > consolidate_reviews.py << 'EOF'
          import json
          import glob
          from pathlib import Path

          # Load all review results
          results = {}
          review_files = glob.glob("review-*.json")

          total_issues = 0
          critical_issues = []
          all_suggestions = []
          scores = {}

          for file in review_files:
              try:
                  review_type = file.replace("review-", "").replace(".json", "")
                  with open(file, 'r') as f:
                      data = json.load(f)
                      results[review_type] = data

                      issues = data.get('issues', [])
                      total_issues += len(issues)

                      # Flag critical issues
                      for issue in issues:
                          if any(word in issue.lower() for word in ['critical', 'security', 'credential', 'injection']):
                              critical_issues.append(f"{review_type}: {issue}")

                      all_suggestions.extend(data.get('suggestions', []))
                      scores[review_type] = data.get('score', 0)

              except Exception as e:
                  print(f"Error processing {file}: {e}")

          # Calculate overall score
          if scores:
              overall_score = sum(scores.values()) // len(scores)
          else:
              overall_score = 0

          # Generate final report
          report = {
              "overall_score": overall_score,
              "total_issues": total_issues,
              "critical_issues": critical_issues,
              "top_suggestions": all_suggestions[:10],
              "detailed_results": results,
              "status": "PASS" if overall_score >= 75 and not critical_issues else "NEEDS_WORK"
          }

          # Save consolidated report
          with open("consolidated-review.json", "w") as f:
              json.dump(report, f, indent=2)

          # Generate markdown report
          with open("code-review-summary.md", "w") as f:
              f.write(f"""# Automated Code Review Summary

          ## Overall Score: {overall_score}/100

          **Status**: {'✅ APPROVED' if report['status'] == 'PASS' else '❌ NEEDS WORK'}

          ## Issues Found: {total_issues}

          ### Critical Issues ({len(critical_issues)})
          """)

              if critical_issues:
                  for issue in critical_issues:
                      f.write(f"- 🔴 {issue}\n")
              else:
                  f.write("- ✅ No critical issues found\n")

              f.write(f"""
          ### Review Scores by Category
          """)

              for category, score in scores.items():
                  emoji = "🔴" if score < 60 else "⚠️" if score < 75 else "✅"
                  f.write(f"- {emoji} **{category.title()}**: {score}/100\n")

              f.write(f"""
          ### Top Suggestions
          """)

              for i, suggestion in enumerate(all_suggestions[:5], 1):
                  f.write(f"{i}. 💡 {suggestion}\n")

              f.write("""
          ---
          *Generated by AWS Labs CI Pipeline - Automated Code Review*
          """)

          print(f"📊 Overall Score: {overall_score}/100")
          print(f"🔍 Total Issues: {total_issues}")
          print(f"🔴 Critical Issues: {len(critical_issues)}")
          print(f"📝 Status: {report['status']}")
          EOF

          python consolidate_reviews.py

      - name: Check for file naming issues
        run: |
          echo "📝 Checking for problematic file naming..."

          # Check for version-control naming patterns
          PROBLEMATIC_FILES=$(find ${TARGET_PATH} -type f \( \
            -name "*_original.*" -o \
            -name "*_new.*" -o \
            -name "*_updated.*" -o \
            -name "*_enhanced.*" -o \
            -name "*_fixed.*" -o \
            -name "*_v[0-9]*.*" -o \
            -name "*_copy.*" -o \
            -name "*_backup.*" -o \
            -name "*.orig" -o \
            -name "*.bak" \
          \) 2>/dev/null || echo "")

          if [ -n "$PROBLEMATIC_FILES" ]; then
            echo "❌ Found files with version-control naming:"
            echo "$PROBLEMATIC_FILES" | sed 's/^/  - /'
            echo ""
            echo "💡 Use git branches and commits for version control instead:"
            echo "  git checkout -b fix/update-feature"
            echo "  git commit -m 'Update feature implementation'"
            echo ""

            # Create issue in consolidated report
            echo "file_naming_issues=true" >> $GITHUB_ENV
          else
            echo "✅ File naming follows best practices"
            echo "file_naming_issues=false" >> $GITHUB_ENV
          fi

      - name: Check directory structure
        run: |
          echo "📁 Validating directory structure..."

          cd "${TARGET_PATH}"

          # Count files in project root
          ROOT_FILES=$(find . -maxdepth 1 -type f | wc -l)

          # List non-essential files in root
          NON_ESSENTIAL=$(find . -maxdepth 1 -type f ! \( \
            -name "README.md" \
            -name "pyproject.toml" \
            -name "LICENSE" \
            -name "NOTICE" \
            -name ".gitignore" \
            -name "Dockerfile" \
            -name "docker-healthcheck.sh" \
            -name "CHANGELOG.md" \
            -name "CONTRIBUTING.md" \
            -name "MANIFEST.in" \
            -name ".python-version" \
            -name ".pre-commit-config.yaml" \
            -name "uv.lock" \
            -name "*requirements*" \
          \) | head -10)

          if [ -n "$NON_ESSENTIAL" ]; then
            echo "⚠️ Non-essential files in project root:"
            echo "$NON_ESSENTIAL" | sed 's/^/  - /'
            echo ""
            echo "💡 Consider organizing into subdirectories:"
            echo "  - src/ for source code"
            echo "  - tests/ for test files"
            echo "  - docs/ for documentation"
            echo "  - scripts/ for utility scripts"
            echo ""
          else
            echo "✅ Project root contains only essential files"
          fi

          echo "structure_clean=$( [ -z \"$NON_ESSENTIAL\" ] && echo 'true' || echo 'false' )" >> $GITHUB_ENV

      - name: Create PR comment (if applicable)
        if: github.event_name == 'pull_request'
        run: |
          echo "💬 Creating PR comment with review results..."

          # Read consolidated results
          if [ -f "consolidated-review.json" ]; then
            SCORE=$(jq -r '.overall_score' consolidated-review.json)
            STATUS=$(jq -r '.status' consolidated-review.json)
            CRITICAL_COUNT=$(jq -r '.critical_issues | length' consolidated-review.json)

            # Create comment body
            cat > pr-comment.md << EOF
          ## 🤖 Automated Code Review Results

          **Overall Score**: $SCORE/100
          **Status**: $( [ "$STATUS" = "PASS" ] && echo "✅ Approved" || echo "❌ Needs Work" )
          **Critical Issues**: $CRITICAL_COUNT

          ### Quick Summary
          - **Security Review**: ${{ matrix.review-type == 'security' && '✅ Completed' || '⏳ In Progress' }}
          - **Quality Review**: ${{ matrix.review-type == 'quality' && '✅ Completed' || '⏳ In Progress' }}
          - **Structure Review**: ${{ matrix.review-type == 'structure' && '✅ Completed' || '⏳ In Progress' }}
          - **Documentation Review**: ${{ matrix.review-type == 'documentation' && '✅ Completed' || '⏳ In Progress' }}

          ### File Naming Compliance
          $( [ "$file_naming_issues" = "false" ] && echo "✅ No version-control naming issues" || echo "❌ Version-control naming issues found" )

          ### Directory Structure
          $( [ "$structure_clean" = "true" ] && echo "✅ Clean directory structure" || echo "⚠️ Directory structure could be improved" )

          <details>
          <summary>📋 Detailed Results</summary>

          \`\`\`json
          $(cat consolidated-review.json)
          \`\`\`

          </details>

          ---
          *Automated review by Claude Sonnet 4 via AWS Labs CI Pipeline*
          EOF

          echo "📝 PR comment prepared (would be posted in real GitHub environment)"
          cat pr-comment.md
          fi

      - name: Upload consolidated results
        uses: actions/upload-artifact@v4
        with:
          name: consolidated-review-results
          path: |
            consolidated-review.json
            code-review-summary.md
            pr-comment.md
          retention-days: 30

  review-gate:
    name: 🚦 Review Gate
    runs-on: ubuntu-latest
    needs: [consolidate-reviews]
    if: always()

    steps:
      - name: Download consolidated results
        uses: actions/download-artifact@v4
        with:
          name: consolidated-review-results

      - name: Evaluate review gate
        run: |
          echo "🚦 Evaluating review gate criteria..."

          if [ -f "consolidated-review.json" ]; then
            SCORE=$(jq -r '.overall_score' consolidated-review.json)
            STATUS=$(jq -r '.status' consolidated-review.json)
            CRITICAL_COUNT=$(jq -r '.critical_issues | length' consolidated-review.json)

            echo "📊 Final Results:"
            echo "  Overall Score: $SCORE/100"
            echo "  Status: $STATUS"
            echo "  Critical Issues: $CRITICAL_COUNT"

            # Set gate status
            if [ "$STATUS" = "PASS" ] && [ "$CRITICAL_COUNT" -eq 0 ] && [ "$SCORE" -ge 75 ]; then
              echo "✅ REVIEW GATE PASSED"
              echo "review_passed=true" >> $GITHUB_ENV
              exit 0
            else
              echo "❌ REVIEW GATE FAILED"
              echo "review_passed=false" >> $GITHUB_ENV

              echo ""
              echo "🔧 To pass the review gate:"
              echo "  1. Fix all critical issues"
              echo "  2. Achieve score ≥ 75/100"
              echo "  3. Address file naming and structure issues"
              echo "  4. Ensure comprehensive documentation"

              exit 1
            fi
          else
            echo "⚠️ No review results found - allowing passage"
            exit 0
          fi
